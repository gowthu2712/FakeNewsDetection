{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import urllib, urllib2\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor:\n",
    "    @classmethod\n",
    "    def get_shallow_POS(self, data):\n",
    "        \"\"\"\n",
    "        Get shallow part of speech tags to identify the sentence's grammatical structure\n",
    "        \"\"\"\n",
    "        pos_tags = []\n",
    "        for sentence in data:\n",
    "            pos_tags.append(Counter(nltk.pos_tag(sentence)))\n",
    "        \n",
    "        return pos_tags\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ngrams(self, data, n):\n",
    "        \"\"\"\n",
    "        Get ngram feature vector which helps to get the context of the sentence \n",
    "        \"\"\"\n",
    "        ngram_vect = []\n",
    "        for sentence in data:\n",
    "            ngram_vect.append(ngrams(Utilities.stem_text(sentence), n))\n",
    "        return ngram_vect\n",
    "    \n",
    "    @classmethod\n",
    "    def get_tf_idf(self, data):\n",
    "        \"\"\"\n",
    "        Get tf_idf which represents the characteristic words in fake and true news classes\n",
    "        \"\"\"\n",
    "        data_tokens = Utilities.stem_text(data)\n",
    "        tfidf = TfidfVectorizer(min_df=1)\n",
    "        tfidf_vect = tfidf.fit(Counter(data_tokens)).transform(data)\n",
    "        return tfidf_vect\n",
    "    \n",
    "    @classmethod\n",
    "    def get_online_relevance_score(self, data):\n",
    "        \"\"\"\n",
    "        Get online relevance score which is the jaccardian similarity between the given data item and bing search results\n",
    "        \"\"\"\n",
    "        online_relevance_score = []\n",
    "        for sentence in data:\n",
    "            bing_results = Scraper.bing_search(sentence)\n",
    "            similarity_sum = 0\n",
    "            for i in range(len(bing_results)) :\n",
    "                results_text = Utilities.stem_text(nltk.word_tokenize(bing_results[i]))\n",
    "                similarity_sum += Algorithms.get_jaccard_similarity(sentence,results_text)\n",
    "            similarity_sum = similarity_sum*1.0/(len(bing_results))\n",
    "            online_relevance_score.append(similarity_sum)\n",
    "        return online_relevance_score\n",
    "        \n",
    "    @classmethod    \n",
    "    def get_feature_vectors(self, data):\n",
    "        \"\"\"\n",
    "        Translates data to feature vectors\n",
    "        \"\"\"\n",
    "        ngram_vector = FeatureExtractor.get_ngrams(data, n=2)\n",
    "        tfidf_vector = FeatureExtractor.get_tf_idf(data)\n",
    "        shallow_pos_vector = FeatureExtractor.get_shallow_POS(data)\n",
    "        online_relevance_score_vector = FeatureExtractor.get_online_relevance_score(data)\n",
    "        features = hstack([ngram_vector, tfidf_vector, shallow_pos_vector, online_relevance_score_vector])\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, nof_trees=10):\n",
    "        self.model = RandomForestClassifier(n_estimators=nof_trees)\n",
    "        \n",
    "    def train(self, training_data, training_label):\n",
    "        \"\"\"\n",
    "        Trains the RF model using the features defined under FeatureExtractor\n",
    "        \"\"\"\n",
    "        training_feature_vector = FeatureExtractor.get_feature_vectors(training_data)\n",
    "        self.model.fit(training_feature_vector, training_label) \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Predicts the output (fake or not) using the trained RF model \n",
    "        \"\"\"\n",
    "        test_feature_vector = FeatureExtractor.get_feature_vectors(test_data)\n",
    "        predict_test = self.model.predict(test_data)\n",
    "        return predict_test\n",
    "    \n",
    "    def get_overall_accuracy(self, prediction, test_label):\n",
    "        \"\"\"\n",
    "        Computes the overall accuracy of the model\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        for i in xrange(len(prediction)):\n",
    "            if prediction[i] == test_label[i]:\n",
    "                accuracy += 1\n",
    "        return accuracy*1.0/len(prediction)\n",
    "    \n",
    "    def get_classification_accuracy(self, prediction, test_label):\n",
    "        \"\"\"\n",
    "        Computes classification accuracy which explains the accuracy of the model for each class - Fake news and True news\n",
    "        \"\"\"\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        POS = 0\n",
    "        NEG = 0\n",
    "\n",
    "        for i in xrange(len(prediction)):\n",
    "            if prediction[i] == test_label[i] and prediction[i] == 1:\n",
    "                TP += 1\n",
    "            elif prediction[i] == test_label[i] and prediction[i] == 0:\n",
    "                TN += 1\n",
    "            if prediction[i] == 1:\n",
    "                POS += 1\n",
    "            if prediction[i] == 0:\n",
    "                NEG += 1\n",
    "\n",
    "        TPR = TP*1.0/(POS)\n",
    "        TNR = TN*1.0/(NEG)\n",
    "\n",
    "        class_accuracy = (TP+TN)*1.0/(len(prediction))\n",
    "        return (TPR, TNR, class_accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Utilities:\n",
    "    @classmethod\n",
    "    def is_ascii(self, s):\n",
    "        \"\"\"\n",
    "        Checks if the string is ASCII or not\n",
    "        \"\"\"\n",
    "        return all(ord(c) < 128 for c in s)\n",
    "    \n",
    "    @classmethod\n",
    "    def read_file(self, file, label):\n",
    "        data = []\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                data.append({\"title\":line, \"label\": label})\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def read_CSV(self, file, delimit, label):\n",
    "        df = pd.read_csv(file, delimiter=delimit, encoding=\"utf-8\")\n",
    "        df['label'] = label\n",
    "        return df[[\"title\", \"label\"]]\n",
    "    \n",
    "    @classmethod\n",
    "    def stem_text(self, text):\n",
    "        text = text.replace('\\n','')\n",
    "        for punc in string.punctuation:\n",
    "            text = text.replace(punc, '')\n",
    "        for num in \"0123456789\":\n",
    "            text = text.replace(num, \"\")\n",
    "        tokenized_text = nltk.word_tokenize(text)\n",
    "        stemmed_text = [x.lower() for x in tokenized_text if x.lower() not in stopwords.words(\"english\")]\n",
    "        return stemmed_text\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DatasetProcessor:\n",
    "    \"\"\"\n",
    "    Processes the true and fake news files and generate training and test data set as specified by \"SPLIT\"\n",
    "    \"\"\"\n",
    "    SPLIT = 0.20\n",
    "    \n",
    "    @classmethod\n",
    "    def get_cleaned_data(self, data, index):\n",
    "        \"\"\"\n",
    "        Data cleaning to remove irrelevant data items for example,\n",
    "        To remove all the float type data which gives no information on textual features\n",
    "        \"\"\"\n",
    "        if type(data['title'].iloc[index]) != type(0.5) and Utilities.is_ascii(data['title'].iloc[index]):\n",
    "            text = ''.join(k for k in data['title'].iloc[index] if not k.isdigit() and type(k) != type(0.5))\n",
    "            if len(text) > 10:\n",
    "                return text\n",
    "        return \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_dataset(self, true_news_file, fake_news_file):\n",
    "        true_dataset = []\n",
    "        fake_dataset = []\n",
    "        \n",
    "        for filename, is_csv in true_news_file:\n",
    "            #True news label is 0\n",
    "            if is_csv:\n",
    "                true_dataset.append(Utilities.read_CSV(file=filename, delimit=',', label=0))\n",
    "            else:\n",
    "                true_dataset.append(Utilities.read_file(file=filename, label=0))\n",
    "\n",
    "        for filename, is_csv in fake_news_file:\n",
    "            #Fake news label is 1\n",
    "            if is_csv:\n",
    "                fake_dataset.append(Utilities.read_CSV(file=filename, delimit=',', label=1))\n",
    "            else:\n",
    "                fake_dataset.append(Utilities.read_file(file=filename, label=1))\n",
    "        \n",
    "        #Generate equal samples of both classes for training to avoid bias towards a particular class\n",
    "        min_count_in_each_class = min(len(true_dataset), len(fake_dataset))\n",
    "        dataset = [true_dataset[i] for i in sorted(random.sample(xrange(len(true_dataset)), min_count_in_each_class))]\n",
    "        dataset += [fake_dataset[i] for i in sorted(random.sample(xrange(len(fake_dataset)), min_count_in_each_class))]\n",
    "        \n",
    "        data = pd.concat(dataset)\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        training_data = []\n",
    "        training_label = []\n",
    "        test_data = []\n",
    "        test_label = []\n",
    "\n",
    "\n",
    "        for i in xrange(int(len(data)*DatasetProcessor.SPLIT)+1):\n",
    "            text = self.get_cleaned_data(data, i)\n",
    "            if text != \"\":\n",
    "                test_data.append(text)\n",
    "                test_label.append(data['label'].iloc[i])\n",
    "\n",
    "        for i in xrange(int(len(data)*DatasetProcessor.SPLIT)+1, len(data)):\n",
    "            text = self.get_cleaned_data(data, i)\n",
    "            if text != \"\":\n",
    "                training_data.append(text)\n",
    "                training_label.append(data['label'].iloc[i])\n",
    "\n",
    "        return (training_data, training_label, test_data, test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @classmethod\n",
    "    def get_jaccard_similarity(self, text1, text2):\n",
    "        intersection_cardinality = len(set.intersection(*[set(text1), set(text2)]))\n",
    "        union_cardinality = len(set.union(*[set(text1), set(text2)]))\n",
    "        return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    @classmethod\n",
    "    def bing_search(self, query):\n",
    "        address = \"http://www.bing.com/search?q=%s\" % (urllib.quote_plus(query))\n",
    "        request = urllib2.Request(address, None, {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11'})\n",
    "        urlfile = urllib2.urlopen(request)\n",
    "        page = urlfile.read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        data = []\n",
    "        for li in soup.findAll('div', attrs={'class':'b_caption'}):\n",
    "            p = li.find('p')\n",
    "            while p.span:\n",
    "                p.span.extract()\n",
    "            while p.a:\n",
    "                p.a.extract()\n",
    "            while p.p:\n",
    "                p.p.extract()\n",
    "            p = str(p)\n",
    "            p = str(p.decode('ascii',errors='ignore'))\n",
    "            p = re.sub('<p>&nbsp;&#0183;&#32;' , '',p)\n",
    "            p = re.sub('<strong>' , '',p)\n",
    "            p = re.sub('</strong>' , '',p)\n",
    "            p = re.sub('...</p>','',p)\n",
    "            p = re.sub('<p>','',p)\n",
    "            p = re.sub('&quot;' , '' ,p)\n",
    "            data.append(p)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def __init__(self):\n",
    "        self.model = svm.SVC(gamma=0.001, C=100, kernel='rbf')\n",
    "    \n",
    "    def train_SVM(training_data, training_label):\n",
    "        ngram_train = ngram_vect.fit(Counter(tokens_counter)).transform(training_data)\n",
    "        tfidf_train = tfidf.fit(Counter(tokens_counter)).transform(training_data)\n",
    "        X_train = hstack([ngram_train, tfidf_train])\n",
    "        pos_train = pos_vect.fit(pos_counter).transform(training_data)\n",
    "        X_train = hstack([X_train, pos_train])\n",
    "        self.model.fit(X_train, training_label) \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        test_feature_vector = FeatureExtractor.get_feature_vectors(test_data)\n",
    "        predict_test = self.model.predict(test_data)\n",
    "        return predict_test\n",
    "    \n",
    "    def get_overall_accuracy(self, prediction, test_label):\n",
    "        accuracy = 0\n",
    "        for i in xrange(len(prediction)):\n",
    "            if prediction[i] == test_label[i]:\n",
    "                accuracy += 1\n",
    "        return accuracy*1.0/len(prediction)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Add the files in the following format - (filename, True)  - if its CSV \n",
    "    #otherwise (filename, False)\n",
    "    TRUE_NEWS = [(\"../data/true-headlines\",False)]\n",
    "    FAKE_NEWS = [(\"../data/fake.csv\", True), (\"../data/fake-news\", False)]\n",
    "    \n",
    "    (training_data, training_label, test_data, test_label) = DatasetProcessor.get_dataset(TRUE_NEWS, FAKE_NEWS)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label: Count_0 =  39863  and count_1 =  32723\n",
      "Test label: Count_0 =  8045  and count_1 =  10104\n"
     ]
    }
   ],
   "source": [
    "    count_1 = 0\n",
    "    count_0 = 0\n",
    "    \n",
    "    for i in training_label:\n",
    "        if i == 1:\n",
    "            count_1 += 1\n",
    "        else:\n",
    "            count_0 += 1\n",
    "    print \"Training label: Count_0 = \", count_0, \" and count_1 = \", count_1\n",
    "    \n",
    "    count_1 = 0\n",
    "    count_0 = 0\n",
    "    \n",
    "    for i in test_label:\n",
    "        if i == 1:\n",
    "            count_1 += 1\n",
    "        else:\n",
    "            count_0 += 1\n",
    "    print \"Test label: Count_0 = \", count_0, \" and count_1 = \", count_1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    rf = RandomForest()\n",
    "    rf.train(training_data, training_label)\n",
    "    prediction = rf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR =  0.846013404206  TNR =  0.986572438163  Classification accuracy =  0.945835942392\n"
     ]
    }
   ],
   "source": [
    "    TPR, TNR, class_accuracy = get_classification_accuracy(test_label, prediction)\n",
    "    print \"TPR = \", TPR, \" TNR = \", TNR, \" Classification accuracy = \", class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
